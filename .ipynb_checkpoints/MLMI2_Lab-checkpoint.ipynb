{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb0fbb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msadi\\anaconda3\\lib\\site-packages\\torchaudio\\backend\\utils.py:62: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691454d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7922c105",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263eab98",
   "metadata": {},
   "source": [
    "## Use `dataloader` to get an utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d135fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_dataloader\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a1da109",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_dataloader('train_fbank.json', 1, False)\n",
    "fbank, lens, trans, dur = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1043ef9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The utterance has 3.79525 seconds.\n",
      "Filter bank feature has 378 frames and 23 dimensions.\n",
      "The correspoinding transcription is: sil dh ah z ae n sil s er z l sil b ih s sil t r ey f aa r w er sil d ih f y uw sil th ih ng sil dh ah m sil th r uw sil k eh r f ah l ih f er s sil t sil.\n"
     ]
    }
   ],
   "source": [
    "print('The utterance has {} seconds.'.format(dur[0]))\n",
    "print('Filter bank feature has {} frames and {} dimensions.'.format(\n",
    "    fbank.shape[0], fbank.shape[2]))\n",
    "print('The correspoinding transcription is: {}.'.format(trans[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cc337f",
   "metadata": {},
   "source": [
    "## Plot FBANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2bcbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "feature = np.transpose(fbank[:, 0, :].numpy())\n",
    "plt.clf()\n",
    "plt.figure(figsize=(10,2))\n",
    "plt.imshow(feature, aspect='auto', origin='lower')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff22c4",
   "metadata": {},
   "source": [
    "# CTC model for ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178724d9",
   "metadata": {},
   "source": [
    "## Obtain phoneme output units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b10b7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First find the unique phones in train.json, and then\n",
    "# create a file named vocab.txt, each line in this \n",
    "# file is a unique phone, in total there should be \n",
    "# 40 lines\n",
    "\n",
    "vocab = {}\n",
    "phonemes = []\n",
    "with open(\"vocab.txt\") as f:\n",
    "    for id, text in enumerate(f):\n",
    "        vocab[text.strip()] = id\n",
    "        phonemes.append(text)\n",
    "phonemes = phonemes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "288f2cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times each phone appear in the training set\n",
    "# and save the numbers correcponding to each phone in vocab.txt \n",
    "# into counts.txt\n",
    "\n",
    "counts = []\n",
    "with open(\"counts.txt\") as f:\n",
    "    for i in f:\n",
    "        counts.append(int(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfd0e54",
   "metadata": {},
   "source": [
    "## Plot frequencies of graphemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63eaf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(10,2))\n",
    "freq = np.array(counts) / np.sum(counts) * 100\n",
    "plt.bar(phonemes, freq)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b3baa",
   "metadata": {},
   "source": [
    "## Model & training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aa93a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "args = {'seed': 123,\n",
    "        'train_json': 'train_fbank.json',\n",
    "        'val_json': 'dev_fbank.json',\n",
    "        'test_json': 'test_fbank.json',\n",
    "        'batch_size': 4,\n",
    "        'num_layers': 1,\n",
    "        'fbank_dims': 23,\n",
    "        'model_dims': 128,\n",
    "        'concat': 1,\n",
    "        'lr': 0.5,\n",
    "        'vocab': vocab,\n",
    "        'report_interval': 50,\n",
    "        'num_epochs': 20,\n",
    "        'device': device,\n",
    "       }\n",
    "\n",
    "args = namedtuple('x', args)(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ffcb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "model = models.BiLSTM(\n",
    "    args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab))\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Total number of model parameters is {}'.format(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec53390d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c3de2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from trainer import train\n",
    "start = datetime.now()\n",
    "model.to(args.device)\n",
    "model_path = train(model, args)\n",
    "end = datetime.now()\n",
    "duration = (end - start).total_seconds()\n",
    "print('Training finished in {} minutes.'.format(divmod(duration, 60)[0]))\n",
    "print('Model saved to {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea487bd",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "056a2234",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can uncomment the following line and change model path to the model you want to decode\n",
    "# model_path=\"checkpoints/20221110_120418/model_16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087018a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print('Loading model from {}'.format(model_path))\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import decode\n",
    "results = decode(model, args, args.test_json)\n",
    "print(\"SUB: {:.2f}%, DEL: {:.2f}%, INS: {:.2f}%, COR: {:.2f}%, PER: {:.2f}%\".format(*results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a0fbf24cf75000eabf5a00e10c8a172f625c080664c667beb546de0043e5ae1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
